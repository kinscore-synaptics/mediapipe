# MediaPipe graph to detect/predict pose landmarks. (CPU input, and inference is
# executed on CPU.) This graph tries to skip pose detection as much as possible
# by using previously detected/predicted landmarks for new images.
#
# It is required that "pose_detection.tflite" is available at
# "mediapipe/modules/pose_detection/pose_detection.tflite"
# path during execution.
#
# It is required that "pose_landmark_full_body.tflite" or
# "pose_landmark_upper_body.tflite" is available at
# "mediapipe/modules/pose_landmark/pose_landmark_full_body.tflite"
# or
# "mediapipe/modules/pose_landmark/pose_landmark_upper_body.tflite"
# path respectively during execution, depending on the specification in the
# UPPER_BODY_ONLY input side packet.
#
# EXAMPLE:
#   node {
#     calculator: "PoseLandmarkCpu"
#     input_side_packet: "UPPER_BODY_ONLY:upper_body_only"
#     input_side_packet: "SMOOTH_LANDMARKS:smooth_landmarks"
#     input_stream: "IMAGE:image"
#     output_stream: "LANDMARKS:pose_landmarks"
#   }

type: "PintoPoseLandmarkCpu"

# CPU image. (ImageFrame)
input_stream: "IMAGE:image"

# Whether to detect/predict the full set of pose landmarks (see below), or only
# those on the upper body. If unspecified, functions as set to false. (bool)
# Note that upper-body-only prediction may be more accurate for use cases where
# the lower-body parts are mostly out of view.
input_side_packet: "UPPER_BODY_ONLY:upper_body_only"

# Pose landmarks within the given ROI. (NormalizedLandmarkList)
# We have 33 landmarks (see pose_landmark_full_body_topology.svg) with the
# first 25 fall on the upper body (see pose_landmark_upper_body_topology.svg),
# and there are other auxiliary key points.
# 0 - nose
# 1 - left eye (inner)
# 2 - left eye
# 3 - left eye (outer)
# 4 - right eye (inner)
# 5 - right eye
# 6 - right eye (outer)
# 7 - left ear
# 8 - right ear
# 9 - mouth (left)
# 10 - mouth (right)
# 11 - left shoulder
# 12 - right shoulder
# 13 - left elbow
# 14 - right elbow
# 15 - left wrist
# 16 - right wrist
# 17 - left pinky
# 18 - right pinky
# 19 - left index
# 20 - right index
# 21 - left thumb
# 22 - right thumb
# 23 - left hip
# 24 - right hip
# 25 - left knee
# 26 - right knee
# 27 - left ankle
# 28 - right ankle
# 29 - left heel
# 30 - right heel
# 31 - left foot index
# 32 - right foot index
#

output_stream: "LANDMARKS:landmarks"
output_stream: "DETECTION:pose_detection"

# Extra outputs (for debugging, for instance).
# Detected poses. (Detection)
output_stream: "unfiltered_detections"
output_stream: "detection_floats"
# Regions of interest calculated based on pose detections. (NormalizedRect)
output_stream: "ROI_FROM_DETECTION:pose_rect_from_detection"
# Regions of interest calculated based on landmarks. (NormalizedRect)
output_stream: "ROI_FROM_LANDMARKS:pose_rect_from_landmarks"


# Transforms the input image into a 128x128 while keeping the aspect ratio
# (what is expected by the corresponding model), resulting in potential
# letterboxing in the transformed image.
node: {
  calculator: "ImageToTensorCalculator"
  input_stream: "IMAGE:image"
  output_stream: "TENSORS:detection_input_tensors"
  output_stream: "LETTERBOX_PADDING:detection_letterbox_padding"
  options: {
    [mediapipe.ImageToTensorCalculatorOptions.ext] {
      output_tensor_width: 128
      output_tensor_height: 128
      keep_aspect_ratio: true
      output_tensor_float_range {
        min: -1.0
        max: 1.0
      }
      border_mode: BORDER_ZERO
      # If this calculator truly operates in the CPU, then gpu_origin is
      # ignored, but if some build switch insists on GPU inference, then we will
      # still need to set this.
      gpu_origin: TOP_LEFT
    }
  }
}

# Calculates size of the image.
node {
  calculator: "ImagePropertiesCalculator"
  input_stream: "IMAGE:image"
  output_stream: "SIZE:image_size"
}

node {
  calculator: "ConstantSidePacketCalculator"
  output_side_packet: "PACKET:pose_detect_model_path"
  options: {
    [mediapipe.ConstantSidePacketCalculatorOptions.ext]: {
      packet {
        string_value: "01_pose_detection/saved_model_tflite_tfjs_coreml_onnx/model_float32.tflite"
      }
    }
  }
}

node {
  calculator: "LocalFileContentsCalculator"
  input_side_packet: "FILE_PATH:pose_detect_model_path"
  output_side_packet: "CONTENTS:pose_detect_model_blob"
  options: {
    [mediapipe.LocalFileContentsCalculatorOptions.ext]: {
      text_mode: false
    }
  }
}

node {
  calculator: "TfLiteModelCalculator"
  input_side_packet: "MODEL_BLOB:pose_detect_model_blob"
  output_side_packet: "MODEL:pose_detect_model"
}

# Runs model inference on CPU.
node {
  calculator: "InferenceCalculator"
  input_side_packet: "MODEL:pose_detect_model"
  input_stream: "TENSORS:detection_input_tensors"
  output_stream: "TENSORS:detection_tensors"
  options: {
    [mediapipe.InferenceCalculatorOptions.ext] {
      delegate { xnnpack {} }
    }
  }
}

node {
  calculator: "TensorsToFloatsCalculator"
  input_stream: "TENSORS:detection_tensors"
  output_stream: "FLOATS:detection_floats"
}

# Generates a single side packet containing a vector of SSD anchors based on
# the specification in the options.
node {
  calculator: "SsdAnchorsCalculator"
  output_side_packet: "anchors"
  options: {
    [mediapipe.SsdAnchorsCalculatorOptions.ext] {
      num_layers: 4
      min_scale: 0.1484375
      max_scale: 0.75
      input_size_height: 128
      input_size_width: 128
      anchor_offset_x: 0.5
      anchor_offset_y: 0.5
      strides: 8
      strides: 16
      strides: 16
      strides: 16
      aspect_ratios: 1.0
      fixed_anchor_size: true
    }
  }
}

# Decodes the detection tensors generated by the TensorFlow Lite model, based on
# the SSD anchors and the specification in the options, into a vector of
# detections. Each detection describes a detected object.
node {
  calculator: "TensorsToDetectionsCalculator"
  input_stream: "TENSORS:detection_tensors"
  input_side_packet: "ANCHORS:anchors"
  output_stream: "DETECTIONS:unfiltered_detections"
  options: {
    [mediapipe.TensorsToDetectionsCalculatorOptions.ext] {
      num_classes: 1
      num_boxes: 896
      num_coords: 12
      box_coord_offset: 0
      keypoint_coord_offset: 4
      num_keypoints: 4
      num_values_per_keypoint: 2
      sigmoid_score: true
      score_clipping_thresh: 100.0
      reverse_output_order: true
      x_scale: 128.0
      y_scale: 128.0
      h_scale: 128.0
      w_scale: 128.0
      min_score_thresh: 0.5
    }
  }
}

# Performs non-max suppression to remove excessive detections.
node {
  calculator: "NonMaxSuppressionCalculator"
  input_stream: "unfiltered_detections"
  output_stream: "filtered_detections"
  options: {
    [mediapipe.NonMaxSuppressionCalculatorOptions.ext] {
      min_suppression_threshold: 0.3
      overlap_type: INTERSECTION_OVER_UNION
      algorithm: WEIGHTED
    }
  }
}

# Adjusts detection locations (already normalized to [0.f, 1.f]) on the
# letterboxed image (after image transformation with the FIT scale mode) to the
# corresponding locations on the same image with the letterbox removed (the
# input image to the graph before image transformation).
node {
  calculator: "DetectionLetterboxRemovalCalculator"
  input_stream: "DETECTIONS:filtered_detections"
  input_stream: "LETTERBOX_PADDING:detection_letterbox_padding"
  output_stream: "DETECTIONS:pose_detections"
}

# Gets the very first detection from "pose_detections" vector.
node {
  calculator: "SplitDetectionVectorCalculator"
  input_stream: "pose_detections"
  output_stream: "pose_detection"
  options: {
    [mediapipe.SplitVectorCalculatorOptions.ext] {
      ranges: { begin: 0 end: 1 }
      element_only: true
    }
  }
}

# Calculates region of interest based on pose detection, so that can be used
# to detect landmarks.
node {
  calculator: "PoseDetectionToRoi"
  input_side_packet: "UPPER_BODY_ONLY:upper_body_only"
  input_stream: "DETECTION:pose_detection"
  input_stream: "IMAGE_SIZE:image_size"
  output_stream: "ROI:pose_rect_from_detection"
}

# Transforms the input image into a 256x256 tensor while keeping the aspect
# ratio (what is expected by the corresponding model), resulting in potential
# letterboxing in the transformed image.
node {
  calculator: "ImageToTensorCalculator"
  input_stream: "IMAGE:image"
  input_stream: "NORM_RECT:pose_rect_from_detection"
  output_stream: "TENSORS:landmarks_input_tensors"
  output_stream: "LETTERBOX_PADDING:landmarks_letterbox_padding"
  options: {
    [mediapipe.ImageToTensorCalculatorOptions.ext] {
      output_tensor_width: 256
      output_tensor_height: 256
      keep_aspect_ratio: true
      output_tensor_float_range {
        min: 0.0
        max: 1.0
      }
    }
  }
}

node {
  calculator: "ConstantSidePacketCalculator"
  output_side_packet: "PACKET:landmarks_model_path"
  options: {
    [mediapipe.ConstantSidePacketCalculatorOptions.ext]: {
      packet {
        string_value: "03_pose_landmark_full_body/saved_model_tflite_tfjs_coreml_onnx/model_float32.tflite"
      }
    }
  }
}

node {
  calculator: "LocalFileContentsCalculator"
  input_side_packet: "FILE_PATH:landmarks_model_path"
  output_side_packet: "CONTENTS:landmarks_model_blob"
  options: {
    [mediapipe.LocalFileContentsCalculatorOptions.ext]: {
      text_mode: false
    }
  }
}

node {
  calculator: "TfLiteModelCalculator"
  input_side_packet: "MODEL_BLOB:landmarks_model_blob"
  output_side_packet: "MODEL:landmarks_model"
}

# Runs model inference on CPU.
node {
  calculator: "InferenceCalculator"
  input_side_packet: "MODEL:landmarks_model"
  input_stream: "TENSORS:landmarks_input_tensors"
  output_stream: "TENSORS:landmarks_output_tensors"
  options: {
    [mediapipe.InferenceCalculatorOptions.ext] {
      delegate { xnnpack {} }
    }
  }
}

# Splits a vector of TFLite tensors to multiple vectors according to the ranges
# specified in option.
node {
  calculator: "SplitTensorVectorCalculator"
  input_stream: "landmarks_output_tensors"
  output_stream: "pose_flag_tensor"
  output_stream: "landmark_tensors"
  options: {
    [mediapipe.SplitVectorCalculatorOptions.ext] {
      ranges: { begin: 0 end: 1 }
      ranges: { begin: 1 end: 2 }
    }
  }
}

node {
  calculator: "TensorsToLandmarksCalculator"
  input_stream: "TENSORS:landmark_tensors"
  output_stream: "NORM_LANDMARKS:raw_landmarks"
  options: {
    [mediapipe.TensorsToLandmarksCalculatorOptions.ext] {
      num_landmarks: 35
      input_image_width: 256
      input_image_height: 256
      visibility_activation: SIGMOID
      presence_activation: SIGMOID
    }
  }
}

# Adjusts landmarks (already normalized to [0.f, 1.f]) on the letterboxed pose
# image (after image transformation with the FIT scale mode) to the
# corresponding locations on the same image with the letterbox removed (pose
# image before image transformation).
node {
  calculator: "LandmarkLetterboxRemovalCalculator"
  input_stream: "LANDMARKS:raw_landmarks"
  input_stream: "LETTERBOX_PADDING:landmarks_letterbox_padding"
  output_stream: "LANDMARKS:adjusted_landmarks"
}

# Projects the landmarks from the cropped pose image to the corresponding
# locations on the full image before cropping (input to the graph).
node {
  calculator: "LandmarkProjectionCalculator"
  input_stream: "NORM_LANDMARKS:adjusted_landmarks"
  input_stream: "NORM_RECT:pose_rect_from_detection"
  output_stream: "NORM_LANDMARKS:all_landmarks"
}

node {
  calculator: "SplitNormalizedLandmarkListCalculator"
  input_stream: "all_landmarks"
  output_stream: "landmarks"
  output_stream: "auxiliary_landmarks"
  options: {
    [mediapipe.SplitVectorCalculatorOptions.ext] {
      ranges: { begin: 0 end: 33 }
      ranges: { begin: 33 end: 35 }
    }
  }
}

# Calculates region of interest based on the auxiliary landmarks, to be used in
# the subsequent image.
node {
  calculator: "PoseLandmarksToRoi"
  input_stream: "LANDMARKS:auxiliary_landmarks"
  input_stream: "IMAGE_SIZE:image_size"
  output_stream: "ROI:pose_rect_from_landmarks"
}
